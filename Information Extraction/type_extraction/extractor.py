"""
Extract type facts from a Wikipedia file


=== Purpose ===

The goal of this lab is to extract the class to which an entity belongs from Wikipedia.
For example, given the Wikipedia article about Leicester:

    Leicester is a small city in England

the goal is to extract:

    Leicester TAB city


=== Provided Data ===

We provide
1. a preprocessed version of the Simple Wikipedia (wikipedia-first.txt), which looks like above
2. a template for your code, extractor.py
3. a gold standard sample (gold-standard-sample.tsv).


=== Task ===

Complete the extract_type() function so that it extracts the type of the article entity from the content.
For example, for a content of "Leicester is a beautiful English city in the UK", it should return "city".
Exclude terms that are too abstract ("member of...", "way of..."), and try to extract exactly the noun(s).
You can also skip articles (e.g. return None) if you are not sure or if the text does not contain any type.
In order to ensure a fair evaluation, do not use any non-standard Python libraries except nltk (pip install nltk).

Input:
April
April is the fourth month of the year with 30 days.

Output:
April TAB month


=== Development and Testing ===

We provide a certain number of gold samples for validating your model.
Finally, we calculate a F1 score using following equation:

F1 = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)

with beta = 0.5, putting more weight on precision in that way.

=== Submission ===

1. Take your code, any necessary resources to run the code, and the output of your code on the test dataset (no need to put the other datasets!)
2. ZIP these files in a file called firstName_lastName.zip
3. submit it here before the deadline announced during the lab:

https://www.dropbox.com/request/Wa81LB15Vdwg8Q9aPZSb
=== Contact ===

If you have any additional questions, you can send an email to: chadi.helwe@telecom-paris.fr

"""

# import NLP packages

import nltk
# nltk.download('averaged_perceptron_tagger')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# import custom packages
from utils import Parsy, eval_f1


# a simplified wiki page document
wiki_file = 'wikipedia-first.txt'
# some gold samples for validation
gold_file = 'gold-standard-sample.tsv'
# predicted results generated by your model
# you are supposed to submit this file
result_file = 'results.tsv'


def extract_type(wiki_page):
    """

    :param wiki_page is an object contains a title and the first sentence from its wiki page.
    :return:
    """
    
    title = wiki_page.title
    content = wiki_page.content

    tokens = word_tokenize(content)
    pos_tag = nltk.pos_tag(tokens)
    #print(pos_tag)
    lenght_pos_tag = len(pos_tag)
    try:
        for i in range(lenght_pos_tag):            
            
            if  (pos_tag[i][1] == 'VB' or pos_tag[i][1] == 'VBZ' or pos_tag[i][1] == 'VBG' or pos_tag[i][1] == 'VBP' or pos_tag[i][1] == 'VBD' or pos_tag[i][1] == 'VBN') and (pos_tag[i+1][1] == 'DT') and (pos_tag[i+2][1] == 'VBZ' or pos_tag[i+2][1] == 'VBG' or pos_tag[i+2][1] == 'VBP' or pos_tag[i+2][1] == 'VBD' or pos_tag[i+2][1] == 'VBN') and (pos_tag[i+3][1] == 'NN' or pos_tag[i+3][1] == 'NNS') and (pos_tag[i+4][1] == 'NN' or pos_tag[i+4][1] == 'NNS'):
                classe = pos_tag[i+4][0]
                break
            
            if  (pos_tag[i][1] == 'VB' or pos_tag[i][1] == 'VBZ' or pos_tag[i][1] == 'VBG' or pos_tag[i][1] == 'VBP' or pos_tag[i][1] == 'VBD' or pos_tag[i][1] == 'VBN') and (pos_tag[i+1][1] == 'DT') and (pos_tag[i+2][1] == 'NN' or pos_tag[i+2][1] == 'NNS') and (pos_tag[i+3][1] == 'IN') and (pos_tag[i+4][1] == 'NN' or pos_tag[i+4][1] == 'NNS'):
                classe = pos_tag[i+4][0]
                break
            
            if (pos_tag[i][1] == 'VB' or pos_tag[i][1] == 'VBZ' or pos_tag[i][1] == 'VBG' or pos_tag[i][1] == 'VBP' or pos_tag[i][1] == 'VBD' or pos_tag[i][1] == 'VBN') and (pos_tag[i+1][1] == 'DT') and (pos_tag[i+2][1] == 'JJ' or pos_tag[i+2][1] == 'JJR' or pos_tag[i+2][1] == 'JJS') and (pos_tag[i+3][1] == 'JJ' or pos_tag[i+3][1] == 'JJR' or pos_tag[i+3][1] == 'JJS') and (pos_tag[i+4][1] == 'NN' or pos_tag[i+4][1] == 'NNS') :
                classe = pos_tag[i+4][0]
                break            
            
            if (pos_tag[i][1] == 'VB' or pos_tag[i][1] == 'VBZ' or pos_tag[i][1] == 'VBG' or pos_tag[i][1] == 'VBP' or pos_tag[i][1] == 'VBD' or pos_tag[i][1] == 'VBN') and (pos_tag[i+1][1] == 'DT') and (pos_tag[i+2][1] == 'JJ' or pos_tag[i+2][1] == 'JJR' or pos_tag[i+2][1] == 'JJS') and (pos_tag[i+3][1] == 'NN' or pos_tag[i+3][1] == 'NNS') :
                classe = pos_tag[i+3][0]
                break
            
            if (pos_tag[i][1] == 'VB' or pos_tag[i][1] == 'VBZ' or pos_tag[i][1] == 'VBG' or pos_tag[i][1] == 'VBP' or pos_tag[i][1] == 'VBD' or pos_tag[i][1] == 'VBN') and (pos_tag[i+1][1] == 'DT') and (pos_tag[i+2][1] == 'NN' or pos_tag[i+2][1] == 'NNS') and (pos_tag[i+3][1] == 'NN' or pos_tag[i+3][1] == 'NNS') and (pos_tag[i+4][1] == 'NN' or pos_tag[i+4][1] == 'NNS') and (pos_tag[i+5][1] != 'NN' or pos_tag[i+5][1] != 'NNS'):
                classe = pos_tag[i+4][0]
                break
                        
            if (pos_tag[i][1] == 'VB' or pos_tag[i][1] == 'VBZ' or pos_tag[i][1] == 'VBG' or pos_tag[i][1] == 'VBP' or pos_tag[i][1] == 'VBD' or pos_tag[i][1] == 'VBN') and (pos_tag[i+1][1] == 'DT') and (pos_tag[i+2][1] == 'NN' or pos_tag[i+2][1] == 'NNS') and (pos_tag[i+3][1] == 'NN' or pos_tag[i+3][1] == 'NNS') and (pos_tag[i+4][1] != 'NN' or pos_tag[i+4][1] != 'NNS'):
                classe = pos_tag[i+3][0]
                break
            
            if (pos_tag[i][1] == 'VBZ' or pos_tag[i][1] == 'VBG' or pos_tag[i][1] == 'VBP' or pos_tag[i][1] == 'VBD' or pos_tag[i][1] == 'VBN') and (pos_tag[i+1][1] == 'DT') and (pos_tag[i+2][1] == 'NN' or pos_tag[i+2][1] == 'NNS') and (pos_tag[i+3][1] != 'NN' or pos_tag[i+3][1] != 'NNS'):
                classe = pos_tag[i+2][0]
                break
            
            if (pos_tag[i][1] == 'VBZ' or pos_tag[i][1] == 'VBG' or pos_tag[i][1] == 'VBP' or pos_tag[i][1] == 'VBD' or pos_tag[i][1] == 'VBN') and (pos_tag[i+1][1] == 'DT') and (pos_tag[i+2][1] == 'NN' or pos_tag[i+2][1] == 'NNS'):
                classe = pos_tag[i+2][0]
                break
                        
            else:
                classe = None
    except:
        classe = None
    
    # Code goes here
    print(classe)

    return classe


def run():
    """
    First, extract types from each sentence in the wiki file
    Next, use gold samples to evaluate your model
    :return:
    """
    """
    entitiesOfInterest=set()
    with open(gold_file, 'r', encoding="utf-8") as input:
        for line in input:
            entitiesOfInterest.add(line.split("\t")[0])
    """
    
    with open(result_file, 'w', encoding="utf-8") as output:
        for page in Parsy(wiki_file):
            print("")
            print(page)
            typ = extract_type(page)
            if typ:
                output.write(page.title + "\t" + typ + "\n")

    # Evaluate on some gold samples for checking your model
    eval_f1(gold_file, result_file)


run()
